{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tutorial: Pose Estimation for Biomechanics (Mobilize & Restore Centers @ Stanford)",
      "provenance": [],
      "collapsed_sections": [
        "PBzVrpDXG2s5",
        "GsH_4sdoQPgB",
        "qRWUKVuuBqqT"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frabjous5/bcrwebsite/blob/master/Tutorial_Pose_Estimation_for_Biomechanics_(Mobilize_%26_Restore_Centers_Stanford).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X38L6tanrnrB"
      },
      "source": [
        "### Mobilize Center & Restore Center @ Stanford Tutorial\n",
        "# Pose Estimation for Biomechanics \n",
        "\n",
        "Quantitative motion analysis is important for the diagnostics of movement disorders and for research. State-of-the-art measurements involve optical motion capture using reflective markers and expensive cameras to capture trajectories of these markers. Though the technique provides high frequency data for assessment, the cost, skills, and time involved limit wide adoption.\n",
        "\n",
        "Recent advancements in deep learning allow us to very robustly detect body landmarks (such as toes, hips, shoulders, etc.) in images from commodity cameras, such as found in smartphones. We can apply these techniques to videos to derive trajectories of landmarks in time. Recent studies show that these trajectories can be used for some clinical applications, potentially reducing the cost of movement analyses by orders of magnitude and facilitating more frequent assessments.\n",
        "\n",
        "##Tutorial Overview\n",
        "**In this notebook we illustrate how to use one deep learning algorithm to analyze human motion. Specifically, we will extract knee flexion curves and gait cycles from a video of a subject walking.** The notebook is for illustrative purposes only, and multiple improvements should be incorporated to obtain accurate knee flexion angles. However, the notebook does provide a pipeline that you can adapt to get started with video-based pose estimation projects.\n",
        "\n",
        "This notebook is a work-in-progress and we welcome your feedback on how to increase its usefulness. Email comments to us at [mobilize-center@stanford.edu](mailto:mobilize-center@stanford.edu).\n",
        "\n",
        "The notebook is a part of the [Mobilize Center](https://mobilize.stanford.edu) webinar series, and is jointly offered with the [Restore Center](https://restore.stanford.edu). The Mobilize Center is an NIH-funded Biomedical Technology Resource Center which provides tools and training to help researchers produce insights from wearables, video, medical images, and other data sources. The Restore Center is an NIH Medical Rehabilitation Research Resource Network Center which is creating a worldwide collaboration to advance the use of real-world data in rehabilitation outcomes for those with movement impairments.\n",
        "\n",
        "##Background and Citation\n",
        "Most of the [code](https://github.com/stanfordnmbl/mobile-gaitlab) in this notebook comes from our study on gait analysis in the cerebral palsy population. To cite our work, please use:\n",
        "\n",
        "*Kidziński, Łukasz, Bryan Yang, Jennifer L. Hicks, Apoorva Rajagopal, Scott L. Delp, and Michael H. Schwartz. \"Deep neural networks enable quantitative movement analysis using single-camera videos.\" Nature communications 11, no. 1 (2020): 1-10.*\n",
        "\n",
        "Learn more about this work:\n",
        "* [Read our publication](https://www.nature.com/articles/s41467-020-17807-z)\n",
        "* Watch the video abstract of this study below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lm4GnG_xGI1b"
      },
      "source": [
        "from IPython.display import YouTubeVideo\n",
        "YouTubeVideo('pb4WvAhsRe4')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBzVrpDXG2s5"
      },
      "source": [
        "## Learning Goals\n",
        "\n",
        "In this tutorial notebook, you will learn how to:\n",
        "\n",
        "* Set up a Google Colab notebook to run OpenPose\n",
        "  - Download OpenPose\n",
        "  - Download a sample video\n",
        "* Run OpenPose on a sample video and work with its output\n",
        "  - Interpret output JSON files\n",
        "  - Convert files to a numpy time series\n",
        "* Preprocess data\n",
        "  - Fix missing data with interpolation\n",
        "  - Fix noise with a Gaussian filter\n",
        "  - Fix bias with normalization\n",
        "* Perform the analysis\n",
        "  - Derive joint angles\n",
        "  - Find gait cycles\n",
        "  - Report results\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xM_Vf8l4f5iE"
      },
      "source": [
        "# Setup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsH_4sdoQPgB"
      },
      "source": [
        "\n",
        "## Introduction to Google Colab\n",
        "\n",
        "Google Colab is a cloud-based environment for running Python code interactively (via Jupyter notebooks, for those who are familiar with those). If you are new to Colab, you can learn about the key features in [this tutorial](https://colab.research.google.com/notebooks/basic_features_overview.ipynb). For the purposes of our tutorial, you only need to understand how to interact with the \"Code cells.\" In the webinar, we also provide a quick overview of how to work with this Colab notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNlQfYZ6Bgm7"
      },
      "source": [
        "\n",
        "## Installation\n",
        "\n",
        "Run the code below to download and compile OpenPose within your Google Colab environment (based on [tugstugi/dl-colab-notebooks](https://github.com/tugstugi/dl-colab-notebooks)). **This takes a while (~15 minutes).**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOdkDhb6ga6N"
      },
      "source": [
        "import os\n",
        "from os.path import exists, join, basename, splitext\n",
        "\n",
        "git_repo_url = 'https://github.com/CMU-Perceptual-Computing-Lab/openpose.git'\n",
        "project_name = splitext(basename(git_repo_url))[0]\n",
        "if not exists(project_name):\n",
        "  # see: https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/949\n",
        "  # install new CMake becaue of CUDA10\n",
        "  !wget -q https://cmake.org/files/v3.13/cmake-3.13.0-Linux-x86_64.tar.gz\n",
        "  !tar xfz cmake-3.13.0-Linux-x86_64.tar.gz --strip-components=1 -C /usr/local\n",
        "  # clone openpose\n",
        "  !git clone -q --depth 1 $git_repo_url\n",
        "  !sed -i 's/execute_process(COMMAND git checkout master WORKING_DIRECTORY ${CMAKE_SOURCE_DIR}\\/3rdparty\\/caffe)/execute_process(COMMAND git checkout f019d0dfe86f49d1140961f8c7dec22130c83154 WORKING_DIRECTORY ${CMAKE_SOURCE_DIR}\\/3rdparty\\/caffe)/g' openpose/CMakeLists.txt\n",
        "  # install system dependencies\n",
        "  !apt-get -qq install -y libatlas-base-dev libprotobuf-dev libleveldb-dev libsnappy-dev libhdf5-serial-dev protobuf-compiler libgflags-dev libgoogle-glog-dev liblmdb-dev opencl-headers ocl-icd-opencl-dev libviennacl-dev\n",
        "  # install python dependencies\n",
        "  !pip install -q youtube-dl\n",
        "  # build openpose\n",
        "  !cd openpose && rm -rf build || true && mkdir build && cd build && cmake .. && make -j`nproc`\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5L3Z5YVrZ2R"
      },
      "source": [
        "## Detect poses in a test video\n",
        "\n",
        "We are going to detect poses on a sample video from a clinic. In this case, the video is stored at https://github.com/stanfordnmbl/mobile-gaitlab/raw/master/demo/in/input.mp4.\n",
        "\n",
        "Click the Files folder icon ![fileFolderIcon.jpg](data:image/jpeg;base64,/9j/4AAQSkZJRgABAgEAkACQAAD//gASTEVBRFRPT0xTIHYyMC4wAP/bAIQABQUFCAUIDAcHDAwJCQkMDQwMDAwNDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQEFCAgKBwoMBwcMDQwKDA0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0N/8QBogAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoLAQADAQEBAQEBAQEBAAAAAAAAAQIDBAUGBwgJCgsQAAIBAwMCBAMFBQQEAAABfQECAwAEEQUSITFBBhNRYQcicRQygZGhCCNCscEVUtHwJDNicoIJChYXGBkaJSYnKCkqNDU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6g4SFhoeIiYqSk5SVlpeYmZqio6Slpqeoqaqys7S1tre4ubrCw8TFxsfIycrS09TV1tfY2drh4uPk5ebn6Onq8fLz9PX29/j5+hEAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/8AAEQgAEQAUAwERAAIRAQMRAf/aAAwDAQACEQMRAD8A7bQvBGs+K7JNXGpMn2oyMVdpSwKyPGc7WxyVJAHABAoA9PttH8WWsSwjULWQRqFDSQMzkDgFmyNx9SeT1JJyaAOF1j4maz4VvZdLvEtbyWAoTKqvGCJI0kA2hu27Ge9AG/4A8WaTpGh21lfXMdvcQmYPG+Qyk3ErAEY4OCDj3oA7L/hPtA/5/oPzP+FAHzb48kGua5dXmnBrm3cwhZIlZlJW3iVhkDqCCCKAPWNd/wCP+f8A66NQBlUAeq+D/wDjw/7aP/SgD//Z) in the left-hand column to see the current files in your virtual Colab environment. After you run the code below, refresh the folder by clicking on the folder icon with the circular arrow ![refreshFolderIcon.jpg](data:image/jpeg;base64,/9j/4AAQSkZJRgABAgEAkACQAAD//gASTEVBRFRPT0xTIHYyMC4wAP/bAIQABQUFCAUIDAcHDAwJCQkMDQwMDAwNDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQEFCAgKBwoMBwcMDQwKDA0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0NDQ0N/8QBogAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoLAQADAQEBAQEBAQEBAAAAAAAAAQIDBAUGBwgJCgsQAAIBAwMCBAMFBQQEAAABfQECAwAEEQUSITFBBhNRYQcicRQygZGhCCNCscEVUtHwJDNicoIJChYXGBkaJSYnKCkqNDU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6g4SFhoeIiYqSk5SVlpeYmZqio6Slpqeoqaqys7S1tre4ubrCw8TFxsfIycrS09TV1tfY2drh4uPk5ebn6Onq8fLz9PX29/j5+hEAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/8AAEQgAGQAhAwERAAIRAQMRAf/aAAwDAQACEQMRAD8A+i/iH4vuPCFpFNaRxySzylP3u4ooCkk4VkJJ4x8wA560AeSxfGvVw6mWCzZARuCpMrFc8hWMzBSR0JVgDzg9KAPSU+L+gsoLGdSQCVMRODjkZBIOOmRx6UAb/h/x5pPiWc2lk7+cql9roUyoIB2nkEjIyM5xyAQDgA7KgDzD4hf8fuif9hSH/wBDjoA9PoAxNa8Q2Xh9Ea8chpTtijRWeWQjGQkagscZGT0GRk5IyAeepr2lah4ktb9HuEu2i+xC1eDy2TcZJBLIZHUhTvIGxX5AzjIoA9doA80+I1rdsdOvbS3luxY3sc8kcKl3KqQ3CgE87cZwQCRmgCP/AIWFe/8AQE1T/vy//wAboAgh1eGPXoNW1WN7KO8sRFbG6Ux+RMkrGaJi4Ajd1OVdsB04VsMFIBLFa6DfeIY5LJXvb1d9xLPHO7w23UqG+Ypl2YhYl6ZyQBQB6jQAUAFAHAfE3/kAz/h/I0Ac78Gf+QXL/wBdv6GgD2GgD//Z). You should see a new file called \"input.mp4\" in your list of files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMx1U-q1bLxy"
      },
      "source": [
        "# Download the video\r\n",
        "!wget https://github.com/stanfordnmbl/mobile-gaitlab/raw/master/demo/in/input.mp4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVfcqKtQbSv_"
      },
      "source": [
        "Let's view the video to see what we will be analyzing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNASdyyiO65I"
      },
      "source": [
        "# Define function to display the video\n",
        "def show_local_mp4_video(file_name, width=640, height=480):\n",
        "  import io\n",
        "  import base64\n",
        "  from IPython.display import HTML\n",
        "  video_encoded = base64.b64encode(io.open(file_name, 'rb').read())\n",
        "  return HTML(data='''<video width=\"{0}\" height=\"{1}\" alt=\"test\" controls>\n",
        "                        <source src=\"data:video/mp4;base64,{2}\" type=\"video/mp4\" />\n",
        "                      </video>'''.format(width, height, video_encoded.decode('ascii')))\n",
        "\n",
        "# Display the video\n",
        "show_local_mp4_video('input.mp4', width=960, height=720)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G38yLmDWcnfY"
      },
      "source": [
        "We will now run OpenPose, specifying our input video file (`input.mp4`) and asking it to output the following:\r\n",
        "\r\n",
        "* The body keypoints in a JSON format in the directory `openpose/output` and \r\n",
        "* A video of the keypoints (`openpose.avi`)\r\n",
        "\r\n",
        "Several other input, output, and run options are available and are described in the [OpenPose documentation](https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/doc/0_index.md).\r\n",
        "\r\n",
        "In the code below, we also convert the output video into an MP4 video (`output.mp4`) for viewing.\r\n",
        "\r\n",
        "NOTE: For these examples, the openpose command must be run from the openpose folder, so that it can locate the supporting files. \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4n8Hp7R8j2h"
      },
      "source": [
        "# delete files from previous runs of this script\n",
        "!rm openpose.avi\n",
        "\n",
        "# detect poses in these video frames using OpenPose\n",
        "!cd openpose && ./build/examples/openpose/openpose.bin --video ../input.mp4 --write_json ./output/ --display 0  --write_video ../openpose.avi\n",
        "\n",
        "# convert the video output result into MP4 so we can visualize it using the 'show_local_mp4_video' function we defined above\n",
        "!ffmpeg -y -loglevel info -i openpose.avi output.mp4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDDkgCCSrFTv"
      },
      "source": [
        "Finally, we can visualize the results: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZ3Ud9zLgOoQ"
      },
      "source": [
        "show_local_mp4_video('output.mp4', width=960, height=720)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_xXtok-6DrM"
      },
      "source": [
        "## View and interpret OpenPose JSON output\n",
        "\n",
        "The output of OpenPose for each frame is saved in a JSON file. \n",
        "\n",
        "You can view the list of output files by clicking on the Files folder openpose -> output. There you will see a list of files named using the format `input_###_keypoints.json` where ### is a number.  \n",
        "\n",
        "Let's print out one of these files to see its structure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3wLiLy_6req"
      },
      "source": [
        "# Load JSON file\n",
        "import json\n",
        "\n",
        "with open('openpose/output/input_000000000100_keypoints.json') as json_file:\n",
        "    data = json.load(json_file)\n",
        "\n",
        "print(json.dumps(data, indent=4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVoWBFGlgUWg"
      },
      "source": [
        "The information in the JSON file is stored in an array named `data`. You access the keypoints in this way: `data[\"people\"][i][\"pose_keypoints_2d\"]`, where `i` is the index of the person (with the first person having an index of 0). Each person is encoded as a 75-dimensional vector, 3 values for each of 25 different keypoints. Columns with indexes `3*k, 3*k+1` and `3*k+2` correspond to `X, Y` and `confidence` of the given keypoint, where `k` is the index of the keypoint (where the first keypoint has an index of 0). Please refer to the figure below for interpretation of the index `k`.\r\n",
        "\r\n",
        "Example: To access the `X` value of the second keypoint (k=1), you would use the following: `data[\"people\"][0][\"pose_keypoints_2d\"][3]` "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ha8OWw6a9ZES"
      },
      "source": [
        "![picture](https://user-images.githubusercontent.com/29478970/76984773-94e34580-693f-11ea-9bcd-7af6fbb1d521.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Cr6x-G7hflR"
      },
      "source": [
        "For clarity, we name all indices using short keywords, for example, an index of 2 refers to the right shoulder (RSHO):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEWMi8efBGnU"
      },
      "source": [
        "NOSE = 0\n",
        "NECK = 1\n",
        "RSHO = 2\n",
        "RELB = 3\n",
        "RWRI = 4\n",
        "LSHO = 5\n",
        "LELB = 6\n",
        "LWRI = 7\n",
        "MHIP = 8\n",
        "RHIP = 9\n",
        "RKNE = 10\n",
        "RANK = 11\n",
        "LHIP = 12\n",
        "LKNE = 13\n",
        "LANK = 14\n",
        "REYE = 15\n",
        "LEYE = 16\n",
        "REAR = 17\n",
        "LEAR = 18\n",
        "LBTO = 19\n",
        "LSTO = 20\n",
        "LHEL = 21\n",
        "RBTO = 22\n",
        "RSTO = 23\n",
        "RHEL = 24\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7W9XCM391C-"
      },
      "source": [
        "## Convert JSON files to a time series of keypoints\n",
        "\n",
        "To run our analysis, we want to load data from all the JSON files into memory. In this case, we name that data array `res`. We will loop through all the files and use the functions from the numpy (np) package to store the subject's data as an array with 344 rows (one for each file/frame) and 75 columns.\n",
        "\n",
        "NOTE: We are using our knowledge/assumption that there is only one person visible in each frame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgVcs41--1OM"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def convert_json2csv(json_directory):\n",
        "    \n",
        "    # initialize res to be array of NaN\n",
        "    res = np.zeros((344,75))\n",
        "    res[:] = np.nan\n",
        "    \n",
        "    # read in JSON files\n",
        "    for frame in range(0,344):\n",
        "        test_image_json = '%sinput_%s_keypoints.json' %            (json_directory, str(frame).zfill(12))\n",
        "\n",
        "        if not os.path.isfile(test_image_json):\n",
        "            break\n",
        "        with open(test_image_json) as data_file:  \n",
        "            data = json.load(data_file)\n",
        "\n",
        "        for person in data['people']:\n",
        "            keypoints = person['pose_keypoints_2d']\n",
        "            xcoords = [keypoints[i] for i in range(len(keypoints)) if i % 3 == 0]\n",
        "            counter = 0\n",
        "            res[frame-1,:] = keypoints\n",
        "            break\n",
        "\n",
        "    # we can save space by dropping rows after the last row that aren't all nan\n",
        "    check = np.apply_along_axis(lambda x: np.any(~np.isnan(x)),1,res)\n",
        "    for i in range(len(check)-1,-1,-1):\n",
        "        if check[i]:\n",
        "            break\n",
        "    return res[:i+1]\n",
        "\n",
        "res = convert_json2csv(\"openpose/output/\")\n",
        "pd.DataFrame(res) # only for clean display"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjsZ6Kf0q4wZ"
      },
      "source": [
        "# Preprocessing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SclqW9CDBj-b"
      },
      "source": [
        "\n",
        "## Diagnostic plots\n",
        "\n",
        "Now we plot some of the trajectories to see whether we need to clean them up using signal processing techniques."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tu0n0YZP_isd"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(res[:,(NOSE*3)])\n",
        "plt.plot(res[:,(NOSE*3+1)])\n",
        "plt.xlabel(\"video frame\")\n",
        "plt.ylabel(\"nose position\")\n",
        "plt.legend(('x','y'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFYxW5tKiYgf"
      },
      "source": [
        "Above we plotted the trajectory of the NOSE keypoint (index = 0). We see that around frame 150, there is some discontinuity. Also, some high frequency noise is present in the signals. Let's check if the same problems show up in curves that we are interested in for computing knee flexion, i.e., trajectories of the right ankle, knee, and hip."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MJs9rxSBDl8"
      },
      "source": [
        "# Features to plot for diagnostics\n",
        "PLOT_COLS = {\n",
        "    \"Right ankle\": RANK,\n",
        "    \"Right knee\": RKNE,\n",
        "    \"Right hip\": RHIP,\n",
        "}\n",
        "\n",
        "# The show_plots function displays a set of curves for each keypoint.\n",
        "# We will use the show_plots again with different data arrays, \n",
        "# so we include the cols_per_point argument to provide flexibility \n",
        "# in specifying how many columns of the array are associated with a \n",
        "# given keypoint.\n",
        "def show_plots(keypoint_array, cols_per_point=3):\n",
        "    for name, col in PLOT_COLS.items():\n",
        "        fig, ax = plt.subplots(figsize=(5, 5))\n",
        "        ax.spines['right'].set_visible(False)\n",
        "        ax.spines['top'].set_visible(False)\n",
        "        plt.title(name,fontsize=24)\n",
        "        plt.xlabel(\"video frame\",fontsize=17)\n",
        "        plt.ylabel(\"position\",fontsize=17)\n",
        "\n",
        "        plt.plot(keypoint_array[:,[col*cols_per_point,]], linestyle=\"-\", linewidth=2.5)\n",
        "        plt.plot(keypoint_array[:,[col*cols_per_point+1,]], linestyle=\"-\", linewidth=2.5)\n",
        "        plt.legend(['x', 'y'],loc=1)\n",
        "\n",
        "show_plots(res)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEqgsntfjElt"
      },
      "source": [
        "We also see discontinuity and some high frequency noise in these signals, so we will clean them up using signal processing techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXNlVmiurDkL"
      },
      "source": [
        "## Clean up signals\n",
        "\n",
        "**Discontinuities and Missing Data:** We will treat the discontinuities like missing data and use linear interpolation to fill in the missing data. There are several other ways to deal with missing data, and we encourage you to explore these for your research projects. You can find many resources online on this topic (search for \"missing data in time series\"), so we will not go over this here. We use linear interpolation because it is straightforward. \n",
        "\n",
        "For the interpolation process, we first want to simplify the data we use. We will copy the JSON output data into the `res_processed` variable, drop the confidence column which we don't need for interpolation, and replace values for undetected keypoints with NaN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8SA_S9_nNOb"
      },
      "source": [
        "# The 3rd column associated with each keypoint in the OpenPose output \n",
        "# is the confidence score. We won't be using it in this notebook so \n",
        "# we can drop it\n",
        "\n",
        "def drop_confidence_cols(keypoint_array):\n",
        "    num_parts = keypoint_array.shape[1]/3 # should give 25 (# of OpenPose keypoints)\n",
        "    processed_cols = [True,True,False] * int(num_parts) \n",
        "    return keypoint_array[:,processed_cols]\n",
        "\n",
        "res_processed = res.copy()\n",
        "res_processed = drop_confidence_cols(res_processed)\n",
        "\n",
        "# if keypoint is not detected, OpenPose returns 0. For undetected keypoints, we change their values to NaN\n",
        "res_processed[res_processed < 0.5] = np.NaN \n",
        "\n",
        "pd.DataFrame(res_processed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSDGuN8IjdaI"
      },
      "source": [
        "You will see that this new data array only has 50 columns, compared with the original 75, reflecting the deletion of the columns with the confidence values. Also, notice that columns 6 and 7 in row 4 now have NaN, replacing the zeros that were originally in columns 9 and 10 of that row.\r\n",
        "\r\n",
        "Next, we replace NaNs with linearly interpolated values and we plot the results to see that the discontinuities are no longer present.\r\n",
        "\r\n",
        "For those interested in understanding the details of the code, the key functions here are:\r\n",
        "\r\n",
        "* `interpolate.interp1d`, which performs the 1D interpolation and is provided through SciPy. The \r\n",
        "full documentation for this function is located [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.interp1d.html).\r\n",
        "*  `np.nanmean`, which computes the mean for an array (ignoring NaN) and is provided through the NumPy package for scientific computing. The full documentation for this function is located [here](https://numpy.org/doc/stable/reference/generated/numpy.nanmean.html?highlight=nanmean#numpy.nanmean)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ntqO_d0su1S"
      },
      "source": [
        "from scipy import interpolate\n",
        "\n",
        "# interpolate to fill nan values\n",
        "def fill_nan(A):\n",
        "    inds = np.arange(A.shape[0]) \n",
        "    good = np.where(np.isfinite(A))\n",
        "    if(len(good[0]) <= 1):\n",
        "        return A\n",
        "   \n",
        "    # linearly interpolate and then fill the extremes with the mean (relatively similar to)\n",
        "    # what kalman does \n",
        "    f = interpolate.interp1d(inds[good], A[good],kind=\"linear\",bounds_error=False)\n",
        "    B = np.where(np.isfinite(A),A,f(inds))\n",
        "    B = np.where(np.isfinite(B),B,np.nanmean(B))\n",
        "    return B\n",
        "    \n",
        "def impute_frames(frames):\n",
        "    return np.apply_along_axis(fill_nan,arr=frames,axis=0)\n",
        "\n",
        "res_processed = impute_frames(res_processed)\n",
        "show_plots(res_processed, 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBxTemzxjmrP"
      },
      "source": [
        "**Noisy Data:** We remove high frequency noise by smoothing the data with a Gaussian filter. Since the observed noise is of low magnitude and high frequency, it is relatively easy to filter out this noise with many popular methods. We chose the Gaussian filter in this tutorial for its simplicity of use.\r\n",
        "\r\n",
        "There are a number of other approaches for removing high frequency noise in a signal. Which is appropriate depends on the nature of your data. These [course notes](https://ggbaker.ca/data-science/content/filtering.html) from Greg Baker at Simon Fraser University provide a useful explanation of some of these other options. \r\n",
        "\r\n",
        "In this example, we use a Gaussian kernel with a standard deviation of 1. The documentation for the `gaussian_filter1d` function we use is [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.gaussian_filter1d.html). \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJZJ593ys1Bv"
      },
      "source": [
        "from scipy.ndimage.filters import gaussian_filter1d\n",
        "\n",
        "# Remove high frequency noise\n",
        "def filter_frames(frames, sd=1):\n",
        "    return np.apply_along_axis(lambda x: gaussian_filter1d(x,sd),\n",
        "                               arr = frames,\n",
        "                               axis = 0)\n",
        "\n",
        "res_processed = filter_frames(res_processed, 1)\n",
        "show_plots(res_processed, 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEJ1fk28jtJr"
      },
      "source": [
        "If you compare these curves with the ones produced in the previous cell, you will notice that these curves are much smoother.\r\n",
        "\r\n",
        "**Bias:** Bias refers to a systemic error. In our case, the manner in which the video was recorded resulted in subjects systemically appearing smaller, then larger, and then smaller again due to the changing camera angle and distance from the subject. This resulted in varying lengths for the body parts that did not accurately reflect reality.\r\n",
        "\r\n",
        "To compensate for this bias, we therefore normalize each frame by (approximately) the length of the subject's femur. The premise is to choose a segment of a fixed length and divide all dimensions by this length. As a result, each pose has exactly the same normalized femur length, and relatively similar dimensions of other segments, since they have approximately fixed relative propotion to the femur in each frame. We chose the femur as it's relatively long, and therefore its measurement is more robust to noise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-H2-qrrJs6yR"
      },
      "source": [
        "# Find mean and standard deviation for debiasing\n",
        "def find_mid_sd(signal_array):\n",
        "    num_parts = signal_array.shape[1]/2 \n",
        "\n",
        "    # Derive mid hip position\n",
        "    mhip_x = ((signal_array[:,2*RHIP] + signal_array[:,2*LHIP])/2).reshape(-1,1)\n",
        "    mhip_y = ((signal_array[:,2*RHIP+1] + signal_array[:,2*LHIP+1])/2).reshape(-1,1)\n",
        "    mhip_coords = np.hstack([mhip_x,mhip_y]*int(num_parts))\n",
        "\n",
        "    # Normalize to hip-knee distance\n",
        "    topoint = lambda x: range(2*x,2*x+2)\n",
        "    scale_vector_R = np.apply_along_axis(lambda x: np.linalg.norm(x[topoint(RHIP)] -\n",
        "                                                                  x[topoint(RKNE)]),1,signal_array)\n",
        "    scale_vector_L = np.apply_along_axis(lambda x: np.linalg.norm(x[topoint(LHIP)] -\n",
        "                                                                  x[topoint(LKNE)]),1,signal_array)\n",
        "    scale_vector = ((scale_vector_R + scale_vector_L)/2.0).reshape(-1,1)\n",
        "    return mhip_coords, scale_vector\n",
        "\n",
        "res_mhip_coords, res_scale_vector = find_mid_sd(res_processed)\n",
        "res_processed = (res_processed-res_mhip_coords)/res_scale_vector\n",
        "show_plots(res_processed, 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PstpdXmkPTj"
      },
      "source": [
        "Note how the y coordinate (orange) in the normalized curve varies around a fairly constant position, compared to the plots in the previous cell without normalization! In the previous cell, the y position (orange) displayed an underlying \"u\" shape."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrz7dmyNrG_a"
      },
      "source": [
        "# Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nB_DJxlaBm6K"
      },
      "source": [
        "\n",
        "## Derive joint angles\n",
        "\n",
        "As we are interested in the knee flexion angle, we will look at the angle formed by the keypoints at the ankle, knee, and hip. We do this by taking the arccosine of the normalized dot product between the ankle-knee vector and the knee-hip vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHb3WTa4tmgF"
      },
      "source": [
        "def get_angle(A,B,C,centered_filtered):\n",
        "   \n",
        "    # finds the angle ABC, assumes that confidence columns have been removed\n",
        "    # A,B and C are integers corresponding to different keypoints\n",
        "    \n",
        "    p_A = np.array([centered_filtered[:,2*A],centered_filtered[:,2*A+1]]).T\n",
        "    p_B = np.array([centered_filtered[:,2*B],centered_filtered[:,2*B+1]]).T\n",
        "    p_C = np.array([centered_filtered[:,2*C],centered_filtered[:,2*C+1]]).T\n",
        "    p_BA = p_A - p_B\n",
        "    p_BC = p_C - p_B\n",
        "    dot_products = np.sum(p_BA*p_BC,axis=1)\n",
        "    norm_products = np.linalg.norm(p_BA,axis=1)*np.linalg.norm(p_BC,axis=1)\n",
        "    return np.arccos(dot_products/norm_products)\n",
        "\n",
        "# get the knee angle and convert from radians to degrees\n",
        "knee_angle = (np.pi - get_angle(RANK, RKNE, RHIP, res_processed))*180/np.pi\n",
        "\n",
        "# plot the knee angle\n",
        "plt.plot(knee_angle)\n",
        "plt.xlabel(\"video frame\")\n",
        "plt.ylabel(\"knee angle (degrees)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_uxItEDrMkJ"
      },
      "source": [
        "## Divide time series into gait cycles\n",
        "\n",
        "As we are interested in the knee flexion over a gait cycle, we will divide the time series into segments. For simplicity, we will define a gait cycle as a *segment between timepoints when a) the distance between the toes on the right and left legs are at a maximum **and** b) the right toes are on the right and the left toes are on the left in the sagittal plane view*.\n",
        "\n",
        "First, we will compute the distance between the toes on the right and left legs and store that in the variable `dst_org`. We then plot that distance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPsLZgRDu99d"
      },
      "source": [
        "def get_distance(A,B,centered_filtered):\n",
        "    p_A = np.array([centered_filtered[:,2*A],centered_filtered[:,2*A+1]]).T\n",
        "    p_B = np.array([centered_filtered[:,2*B],centered_filtered[:,2*B+1]]).T    \n",
        "    p_BA = p_A - p_B\n",
        "    return np.linalg.norm(p_BA,axis=1)\n",
        "\n",
        "dst_org = get_distance(RBTO, LBTO, res_processed) * np.sign(res_processed[:,LBTO*2] - res_processed[:,RBTO*2])\n",
        "plt.plot(dst_org)\n",
        "plt.xlabel(\"video frame\")\n",
        "plt.ylabel(\"distance between right and left toes\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQgKfAWwmUYc"
      },
      "source": [
        "In the plot above, there is some noise around frames 50-100, so we'll smooth the signal using a Gaussian filter as we did previously. The smoothed distance data is stored in the variable `dst`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INXjrbTdwBvE"
      },
      "source": [
        "dst = gaussian_filter1d(dst_org.copy(),5)\n",
        "plt.plot(dst)\n",
        "plt.xlabel(\"video frame\")\n",
        "plt.ylabel(\"distance between right and left toes, smoothed\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7lY33JDmawM"
      },
      "source": [
        "Now we need to detect the maximum distance between the toes. We will do that by finding the peaks, which are clearly visible. We will use a simple algorithm for peak detection and store the indices for the peaks, along with the peak values, in a two-column variable called `maxs`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5mI0O-UwWqM"
      },
      "source": [
        "# Peak detection script converted from MATLAB script\n",
        "# at http://billauer.co.il/peakdet.html\n",
        "#    \n",
        "# Returns two arrays\n",
        "#    \n",
        "# function [maxtab, mintab]=peakdet(v, delta, x)\n",
        "# PEAKDET Detect peaks in a vector\n",
        "# [MAXTAB, MINTAB] = PEAKDET(V, DELTA) finds the local\n",
        "# maxima and minima (\"peaks\") in the vector V.\n",
        "# MAXTAB and MINTAB consists of two columns. Column 1\n",
        "# contains indices in V, and column 2 the found values.\n",
        "#     \n",
        "# With [MAXTAB, MINTAB] = PEAKDET(V, DELTA, X) the indices\n",
        "# in MAXTAB and MINTAB are replaced with the corresponding\n",
        "# X-values.\n",
        "#\n",
        "# A point is considered a maximum peak if it has the maximal\n",
        "# value, and was preceded (to the left) by a value lower by\n",
        "# DELTA.\n",
        "#\n",
        "# Eli Billauer, 3.4.05 (Explicitly not copyrighted).\n",
        "# This function is released to the public domain; Any use is allowed.\n",
        "def peakdet(v, delta, x = None):\n",
        "    maxtab = []\n",
        "    mintab = []\n",
        "       \n",
        "    if x is None:\n",
        "        x = np.arange(len(v))\n",
        "    \n",
        "    v = np.asarray(v)\n",
        "    \n",
        "    if len(v) != len(x):\n",
        "        sys.exit('Input vectors v and x must have same length')\n",
        "    \n",
        "    if not np.isscalar(delta):\n",
        "        sys.exit('Input argument delta must be a scalar')\n",
        "    \n",
        "    if delta <= 0:\n",
        "        sys.exit('Input argument delta must be positive')\n",
        "    \n",
        "    mn, mx = np.Inf, -np.Inf\n",
        "    mnpos, mxpos = np.NaN, np.NaN\n",
        "    \n",
        "    lookformax = True\n",
        "    \n",
        "    for i in np.arange(len(v)):\n",
        "        this = v[i]\n",
        "        if this > mx:\n",
        "            mx = this\n",
        "            mxpos = x[i]\n",
        "        if this < mn:\n",
        "            mn = this\n",
        "            mnpos = x[i]\n",
        "        \n",
        "        if lookformax:\n",
        "            if this < mx-delta:\n",
        "                maxtab.append((mxpos, mx))\n",
        "                mn = this\n",
        "                mnpos = x[i]\n",
        "                lookformax = False\n",
        "        else:\n",
        "            if this > mn+delta:\n",
        "                mintab.append((mnpos, mn))\n",
        "                mx = this\n",
        "                mxpos = x[i]\n",
        "                lookformax = True\n",
        "\n",
        "    return np.array(maxtab), np.array(mintab)\n",
        "\n",
        "maxs, mins = peakdet(dst, 0.5)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Zs2eCdcnRiI"
      },
      "source": [
        "We now add vertical lines to the identified peak locations in the plot below. This will allow us to do a visual check to see if the peaks are identified correctly. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8T1XDsTQxKFq"
      },
      "source": [
        "# plot the smoothed distance curve\n",
        "plt.plot(dst)\n",
        "plt.xlabel(\"video frame\")\n",
        "plt.ylabel(\"distance between right and left toes, smoothed\")\n",
        "\n",
        "# plot a vertical line at each point identified to have a maximum\n",
        "for mm in maxs.tolist():\n",
        "    plt.axvline(x=mm[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Xsqafugq7pg"
      },
      "source": [
        "The peaks appear to be identified correctly, so we will use \r\n",
        "these video frames to similarly mark the split between gait cycles on the knee angle time series."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6V7po2Dq65Y"
      },
      "source": [
        "# plot the knee angle curve\n",
        "plt.plot(knee_angle)\n",
        "plt.xlabel(\"video frame\")\n",
        "plt.ylabel(\"knee angle (degrees)\")\n",
        "\n",
        "# plot a vertical line at each split between gait cycles\n",
        "for mm in maxs.tolist():\n",
        "    plt.axvline(x=mm[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2U8YESy5rBoW"
      },
      "source": [
        "We will extract the knee angles for the first 4 full segments, or gait cycles in this case, and store them in the variable `segments` to be used for further analysis and visualization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEm5CK17xu6e"
      },
      "source": [
        "segments = []\n",
        "sstart = int(maxs[0][0])\n",
        "for i in range(1,5):\n",
        "    end = int(maxs[i][0])\n",
        "    segments.append(knee_angle[sstart:end])\n",
        "    print(\"Segment from {} to {}\".format(sstart,end))\n",
        "    sstart = end"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4wlnidotp-q"
      },
      "source": [
        "# Report results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xsd0q6FkBoPg"
      },
      "source": [
        "\n",
        "## Plot knee angle for four gait cycles and export the plot\n",
        "\n",
        "We can now visually compare the knee angle over the first four gait cycles. After you run the cell below, you should see a plot with four curves, one for each gait cycle. Using the `plt.savefig` command, we also exported this plot to a file called \"curves.png\" that should appear in your Files folder to the left."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQ2NiU-eyy10"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(7, 5))\n",
        "ax.spines['right'].set_visible(False)\n",
        "ax.spines['top'].set_visible(False)\n",
        "plt.title(\"Approx. planar knee angle\",fontsize=24)\n",
        "plt.xlabel(\"gait cycle (in %)\",fontsize=17)\n",
        "plt.ylabel(\"angle (in degrees)\",fontsize=17)\n",
        "\n",
        "# Because each gait cycle has a slightly different number of points, \n",
        "# we must interpolate the data for each gait cycle so that\n",
        "# each gait cycle has the same number of points in order to plot\n",
        "# them together. Here we choose 100 points.\n",
        "grid = np.linspace(0.0, 1.0, num=100)\n",
        "curves = np.zeros([4,100])\n",
        "\n",
        "for i in range(4):\n",
        "    org_space = np.linspace(0.0, 1.0, num=int(segments[i].shape[0]))\n",
        "    f = interpolate.interp1d(org_space, segments[i], kind=\"linear\")\n",
        "    plt.plot(grid*100, f(grid), linestyle=\"-\", linewidth=2.5)\n",
        "    plt.legend([\"Cycle {}\".format(k) for k in range(1,5)],loc=1)\n",
        "    curves[i,:] = f(grid)\n",
        "\n",
        "plt.savefig(\"curves.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdSAYpaydoJy"
      },
      "source": [
        "## Export cleaned up trajectories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6DsZkhsa0BO"
      },
      "source": [
        "We can also save any of our data to a text file. Below we save the trajectories to a file called \"trajectories.csv.\" After running the cell below, you should see that file in your Files folder to the left."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NxvMUOrdwrP"
      },
      "source": [
        "np.savetxt(\"trajectories.csv\", curves, delimiter=\",\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRWUKVuuBqqT"
      },
      "source": [
        "# Experiment with the Code\n",
        "\n",
        "Here are a few activities you can try to increase your familiarity with the code. They are ordered from simplest to most complex.\n",
        "\n",
        "* Change the variance of the Gaussian smoothing filter to 5. How does this change the calculated joint angles?\n",
        "* Plot 4 gait cycles of the ankle flexion angle\n",
        "* Compute the mean and variance of peak knee flexion across 4 gait cycles\n",
        "* Run the scripts on another video https://github.com/stanfordnmbl/mobile-gaitlab/blob/master/demo/in/input1.mp4?raw=true. Note that because of people in the background, this video needs more preprocessing. Some of the assumptions we made in the code above will no longer be true.\n",
        "* Install keras and run neural networks from our study (https://github.com/stanfordnmbl/mobile-gaitlab/) to get better predictors of gait parameters from videos. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rO7S3Zu2EIgM"
      },
      "source": [
        "# Other resources\n",
        "\n",
        "* Our publication on our study on gait analysis in Cerebral Palsy population: https://www.nature.com/articles/s41467-020-17807-z\n",
        "* Code from our study and demo of neural networks: https://github.com/stanfordnmbl/mobile-gaitlab\n",
        "* OpenPose repository: https://github.com/CMU-Perceptual-Computing-Lab/openpose\n",
        "* Facebook's software for estimating 3d poses in videos: https://github.com/facebookresearch/VideoPose3D\n",
        "* DeepLabCut, software for creating virtual markers for different body parts in different species (not just humans) from videos: http://www.mousemotorlab.org/deeplabcut\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifTa38Spblu9"
      },
      "source": [
        "\r\n",
        "\r\n",
        "---\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9gidtZddPYg"
      },
      "source": [
        "Version 1.0\r\n",
        "\r\n",
        "Creator: Lukasz Kidzinski | Contributors: Lukasz Kidzinski, Joy Ku  \r\n",
        "Last Updated on 1/27/2020\r\n",
        "\r\n",
        "This notebook is made available under the [Apache 2.0 license](https://www.apache.org/licenses/LICENSE-2.0).\r\n",
        "\r\n",
        "Note that software used within the notebook might have different licenses. Please refer to the documentation for individual software and packages. In particular, OpenPose is free only for non-commercial use (see its [license](https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/LICENSE) for details). For an open-source alternative , see [Detectron2](https://github.com/facebookresearch/detectron2) for a keypoint detection algorithm under Apache 2.0 license."
      ]
    }
  ]
}